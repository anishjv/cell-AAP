import cv2
import numpy as np
import numpy.typing as npt
import torch
import skimage
from skimage.measure import regionprops, label
from skimage.morphology import white_tophat, square, erosion
from skimage.filters import (
    gaussian,
    threshold_isodata,
    threshold_multiotsu,
)  # pylint: disable=no-name-in-module
from typing import Optional, Union
import scipy
import gc  # Add garbage collection import


def preprocess_2d(
    image: npt.NDArray,
    threshold_division: float,
    sigma: float,
    threshold_type: Optional[str] = "single",
    erosionstruct: Optional[any] = False,
    tophatstruct: Optional[any] = square(71),
) -> tuple[npt.NDArray, npt.NDArray]:
    """
    Preprocesses a 2D image with Gaussian smoothing, background subtraction, and thresholding.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	image: npt.NDArray, input image to preprocess
    	threshold_division: float, division factor for threshold calculation
    	sigma: float, sigma value for Gaussian smoothing
    	threshold_type: Optional[str], type of thresholding ("single" or "multi")
    	erosionstruct: Optional[any], structuring element for erosion operation
    	tophatstruct: Optional[any], structuring element for white top-hat operation
    OUTPUTS:
    	labels: npt.NDArray, labeled connected components
    	redseg: npt.NDArray, binary segmented image
    """

    im = gaussian(image, sigma)  # 2D gaussian smoothing filter to reduce noise
    
    if isinstance(tophatstruct, np.ndarray):
        im = white_tophat(im, tophatstruct)
    else:
        pass

    if isinstance(erosionstruct, np.ndarray):
        im = erosion(im, erosionstruct)
    else:
        pass

    if threshold_type == "multi":
        thresholds = threshold_multiotsu(im)
        redseg = np.digitize(im, bins=thresholds)
    else:
        thresh = threshold_isodata(im)
        redseg = im > (thresh / threshold_division)

    labels = label(redseg)

    return labels, redseg


def preprocess_3d(
    targetstack: npt.NDArray,
    threshold_division: float,
    sigma: int,
    threshold_type: Optional[str] = "single",
    erosionstruct: Optional[any] = False,
    tophatstruct: Optional[any] = square(71),
) -> tuple[npt.NDArray, dict]:
    """
    Preprocesses a stack of images and computes region properties for each frame.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	targetstack: npt.NDArray, stack of images with shape (z, n, n)
    	threshold_division: float, division factor for threshold calculation
    	sigma: int, sigma value for Gaussian smoothing
    	threshold_type: Optional[str], type of thresholding to use ("single" or "multi")
    	erosionstruct: Optional[any], structuring element for erosion operation
    	tophatstruct: Optional[any], structuring element for white top-hat operation
    OUTPUTS:
    	labels_whole: npt.NDArray, labeled stack of processed images
    	region_props: dict, region properties for each frame, indexed as 'region_props['Frame_i']'
    """

    region_props = {}
    labels_whole = []

    for i in range(targetstack.shape[0]):
        im = targetstack[i, :, :].copy()

        labels, _ = preprocess_2d(
            im, threshold_division, sigma, threshold_type, erosionstruct, tophatstruct
        )
        region_props[f"Frame_{i}"] = regionprops(labels, intensity_image=labels * im)
        labels_whole.append(labels)
        
        # Clear intermediate variables to reduce memory usage
        del im
        if i % 10 == 0:  # Garbage collect every 10 frames
            gc.collect()

    labels_whole = np.asarray(labels_whole)

    return labels_whole, region_props


def bw_to_rgb(
    image: npt.NDArray,
    max_pixel_value: Optional[int] = 255,
    min_pixel_value: Optional[int] = 0,
) -> npt.NDArray:
    """
    Converts a black and white image to RGB format by replicating the grayscale values across all channels.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	image: npt.NDArray, input grayscale image of shape (x, y)
    	max_pixel_value: Optional[int], maximum pixel value for normalization (default 255)
    	min_pixel_value: Optional[int], minimum pixel value for normalization (default 0)
    OUTPUTS:
    	rgb_image: npt.NDArray, RGB image of shape (x, y, 3) with identical values across all channels
    """
    if len(np.asarray(image).shape) == 2:
        # Normalize image in-place to save memory
        image_normalized = cv2.normalize(
            np.asarray(image),
            None,
            max_pixel_value,
            min_pixel_value,
            cv2.NORM_MINMAX,
            cv2.CV_8U,
        )
        # Use broadcasting for more memory-efficient RGB conversion
        rgb_image = np.stack([image_normalized] * 3, axis=-1)
        
        # Clear intermediate variable
        del image_normalized

    return rgb_image


def get_box_size(
    region_props: skimage.measure.regionprops, scaling_factor: float
) -> float:
    """
    Computes bounding box size for cell segmentation based on region properties.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	region_props: skimage.measure.regionprops, region properties for detected nuclei
    	scaling_factor: float, scaling factor to convert nucleus size to cell size (lÂ²/A where l is ideal box length, A is mean nucleus area)
    OUTPUTS:
    	bb_side_length: float, half the side length of a bounding box
    """

    major_axis = [region_props[i].axis_major_length for i, _ in enumerate(region_props)]

    dna_major_axis = np.median(np.asarray(major_axis))
    bb_side_length = scaling_factor * dna_major_axis

    print("The bounding box side length was", bb_side_length, "pixels")
    return bb_side_length // 2


def get_box_size_scaled(region_props, max_size: float) -> list[float]:
    """
    Computes scaled bounding box sizes based on region properties and intensity.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	region_props: skimage.measure.regionprops, region properties for detected nuclei
    	max_size: float, maximum bounding box size for scaling
    OUTPUTS:
    	bb_side_lengths: list[float], list of half side lengths for bounding boxes, scaled based on nucleus properties
    """

    major_axis = [region_props[i].axis_major_length for i, _ in enumerate(region_props)]
    intensity = [region_props[i].intensity for i, _ in enumerate(region_props)]

    std_intensity = np.std(intensity)
    std_major_axis = np.std(major_axis)
    mean_intensity = np.mean(intensity)
    mean_major_axis = np.mean(major_axis)

    bb_side_lengths = []
    for i, _ in enumerate(region_props):
        z_score = 0.5 * (
            (major_axis[i] - mean_major_axis) / std_major_axis
            + (intensity[i] - mean_intensity) / std_intensity
        )
        percentile = scipy.integrate.quad(
            lambda x: (1 / 2 * np.pi) * np.e ** (-(x**2) / 2), -np.inf, z_score
        )
        bb_side_lengths.append(max_size * percentile)

    print(np.asarray(bb_side_lengths))
    return np.asarray(bb_side_lengths) // 2


def square_box(centroid: list[float], box_size: float) -> npt.NDArray:
    """
    Draws an upright bounding box given a centroid and box size.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	centroid: list[float], centroid coordinates in the form (y, x)
    	box_size: float, half the side length of the bounding box
    OUTPUTS:
    	coords: npt.NDArray, bounding box coordinates in the form [x1, y2, x2, y1] where (x1, y1) is top-left and (x2, y2) is bottom-right
    """

    x, y = centroid[1], centroid[0]  # centroid must be of the form (y, x)
    x1, y1 = x - box_size, y + box_size  # top left
    x2, y2 = x + box_size, y - box_size  # bottom right

    return np.asarray([x1, y2, x2, y1])


def box_size_wrapper(func, frame_props, args):
    """
    Facilitates the usage of different box size determination functions.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	func: callable, function to determine box sizes
    	frame_props: skimage.measure.regionprops, region properties for a frame
    	args: tuple, additional arguments to pass to the function
    OUTPUTS:
    	result: any, result from the box size determination function
    """
    try:
        return func(frame_props, *args)
    except Exception as error:
        raise AttributeError("args do not match function") from error


def bbox_wrapper(
    func, centroid, box_size: Optional[float] = None, args: Optional[list] = None
):
    """
    Facilitates the usage of different box drawing determination functions.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	func: callable, function to draw bounding boxes
    	centroid: npt.NDArray, centroid coordinates in the form (y, x)
    	box_size: Optional[float], half the side length of the bounding box
    	args: Optional[list], additional arguments to pass to the function
    OUTPUTS:
    	result: any, result from the box drawing function
    """
    try:
        if box_size and args:
            return func(centroid, box_size, *args)
        elif box_size:
            return func(centroid, box_size)
        elif args:
            return func(centroid, *args)
        else:
            return func(centroid)
    except Exception as error:
        raise AttributeError("args do not match function") from error


def iou_with_list(
    input_list: list[npt.NDArray], iou_thresh: float
) -> list[int]:
    """
    Return indices (w.r.t. original order) of non-overlapping masks (IoU < iou_thresh);
    preferentially keep the larger mask. Uses PyTorch NMS for optimal speed.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	input_list: list[npt.NDArray], list of packed binary masks
    	iou_thresh: float, IoU threshold above which masks are considered overlapping
    OUTPUTS:
    	kept_idx: list[int], indices of masks to keep (in original order) to eliminate overlaps
    """

    # Precompute popcount table for fast area computation
    _POPCOUNT = np.array([bin(i).count("1") for i in range(256)], dtype=np.uint8)

    def _area_packed(mask: np.ndarray) -> int:
        m = mask.view(np.uint8)
        return int(_POPCOUNT[m].astype(np.uint32).sum())

    def _bbox_from_packed(mask: np.ndarray) -> tuple[int, int, int, int]:
        """Get bounding box from packed mask"""
        # Unpack just enough to get bounding box
        mask_bool = np.unpackbits(mask.view(np.uint8), axis=0).astype(bool, copy=False)
        rows = np.any(mask_bool, axis=1)
        cols = np.any(mask_bool, axis=0)
        y1, y2 = np.where(rows)[0][[0, -1]] if len(rows) > 0 else (0, 0)
        x1, x2 = np.where(cols)[0][[0, -1]] if len(cols) > 0 else (0, 0)
        return x1, y1, x2, y2

    if not input_list:
        return []

    # Compute areas and bounding boxes
    areas = np.array([_area_packed(m) for m in input_list], dtype=np.float32)
    bboxes = [_bbox_from_packed(m) for m in input_list]
    
    # Convert to PyTorch tensors
    boxes_tensor = torch.tensor(bboxes, dtype=torch.float32)
    scores_tensor = torch.tensor(areas, dtype=torch.float32)
    
    # Use PyTorch NMS - it keeps the highest scoring boxes (largest areas)
    # NMS returns indices of kept boxes
    kept_indices = torch.ops.torchvision.nms(
        boxes_tensor, 
        scores_tensor, 
        iou_threshold=iou_thresh
    ).numpy()
    
    # Convert to list of kept indices in original order
    kept_idx = kept_indices.tolist()

    return kept_idx


def predict(
    predictor,
    image,
    boxes: Optional[list[list]] = None,
    points: Optional[list] = None,
    box_prompts=False,
    point_prompts=True,
    point_labels: Optional[list] = None,
) -> np.ndarray:
    """
    Implementation of FAIR's SAM using box or point prompts.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	predictor: SAM predictor object, predictive algorithm for segmenting cells
    	image: npt.NDArray, input image for segmentation
    	boxes: Optional[list[list]], list of bounding box coordinates
    	points: Optional[list], list of point coordinates
    	box_prompts: bool, whether to use box prompts for segmentation
    	point_prompts: bool, whether to use point prompts for segmentation
    	point_labels: Optional[list], labels for point prompts (1 for positive, 0 for negative)
    OUTPUTS:
    	segmentations: np.ndarray, packed binary masks from SAM prediction
    """
    segmentations = []
    if box_prompts == True:

        try:
            assert boxes != None
        except Exception as error:
            raise AssertionError(
                "Must provide input bounding boxes if box_propmts = True has been selected"
            ) from error

        input_boxes = torch.tensor(boxes, device=predictor.device)
        # Get image shape from predictor if image is None
        if image is None:
            # SAM already has the image set, get shape from predictor
            image_shape = predictor.original_size
        else:
            image_shape = image.shape[:2]
            
        transformed_boxes = predictor.transform.apply_boxes_torch(
            input_boxes, image_shape
        )
        masks, _, _ = predictor.predict_torch(
            point_coords=None,
            point_labels=None,
            boxes=transformed_boxes,
            multimask_output=False,
        )

        masks = masks.detach().cpu().numpy()

    elif point_prompts == True:

        if not point_labels:
            point_labels = np.ones(len(points))

        try:
            assert points != None
        except Exception as error:
            raise AssertionError(
                "Failed to provide input centroids, please select box_prompts = True if attempting to provide bouding box prompts"
            ) from error
        masks, _, _ = predictor.predict(
            point_coords=np.array(points),
            point_labels=point_labels,
            box=None,
            multimask_output=False,
        )

    if len(masks.shape) == 4:
        for h in range(masks.shape[0]):
            packed_mask = np.packbits(masks[h, 0, :, :], axis=0)
            segmentations.append(packed_mask)

    else:
        segmentations = np.packbits(masks[0, :, :], axis=0)

    return segmentations


def crop_regions_predict(
    dna_image_stack,
    phase_image_stack,
    predictor,
    threshold_division: float,
    sigma: float,
    erosionstruct,
    tophatstruct,
    box_size: tuple,
    point_prompts: bool = True,
    box_prompts: bool = False,
    to_segment: bool = True,
    threshold_type: str = "single",
    iou_thresh: Optional[float] = 0.85,
):
    """
    Crops regions from DNA and phase microscopy images and generates cell segmentations.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	dna_image_stack: npt.NDArray, array of shape (image_count, x, y) where each image corresponds to one DNA image
    	phase_image_stack: npt.NDArray, array of shape (image_count, x, y) where each image corresponds to one phase image
    	predictor: SAM predictor object, predictive algorithm for segmenting cells
    	threshold_division: float, division factor for threshold calculation
    	sigma: float, sigma value for Gaussian smoothing
    	erosionstruct: any, structuring element for erosion operation
    	tophatstruct: any, structuring element for white top-hat operation
    	box_size: tuple, tuple containing (function, args) for box size determination
    	point_prompts: bool, whether to use point prompts for segmentation
    	box_prompts: bool, whether to use box prompts for segmentation
    	to_segment: bool, whether to perform segmentation
    	threshold_type: str, type of thresholding to use ("single" or "multi")
    	iou_thresh: Optional[float], IoU threshold for removing overlapping masks
    OUTPUTS:
    	dna_regions: npt.NDArray, rank 4 tensor of cropped ROI's indexed as dna_regions[frame][cell]
    	discarded_box_counter: npt.NDArray, vector of integers corresponding to the number of ROI's discarded due to incomplete bounding boxes
    	segmentations: npt.NDArray, rank 4 tensor containing one mask per cell per frame, indexed as segmentations[frame][cell]
    	phs_regions: npt.NDArray, rank 4 tensor of cropped phase image ROI's
    	dna_seg: npt.NDArray, regions containing nucleus masks
    """

    try:
        assert dna_image_stack.shape[0] == phase_image_stack.shape[0]
    except Exception as error:
        raise AssertionError(
            "there must be the same number of frames in the dna image and the corresponding phase image"
        ) from error

    try:
        assert box_prompts != point_prompts
    except Exception as error:
        raise AssertionError(
            "You must use only one of box prompts and point prompts"
        ) from error

    # Reduce batch size to minimize memory usage
    batch_size = 10  # Reduced from 50
    discarded_box_counter = np.array([])
    dna_regions = []
    phs_regions = []
    segmentations = []
    dna_seg = []
    prompts = []
    boxes = []
    box_size_func = box_size[0]
    box_size_args = box_size[1]

    # Process images one at a time to reduce memory usage
    labeled_stack, dna_image_region_props = preprocess_3d(
        dna_image_stack,
        threshold_division,
        sigma,
        threshold_type,
        erosionstruct,
        tophatstruct,
    )

    for i, _ in enumerate(dna_image_region_props):  # for each image

        frame_props = dna_image_region_props[f"Frame_{i}"]
        box_sizes = box_size_wrapper(box_size_func, frame_props, box_size_args)
        dna_regions_temp = []
        phs_regions_temp = []
        dna_seg_temp = []
        segmentations_temp = []
        prompts_temp = []
        discarded_box_counter = np.append(discarded_box_counter, 0)
        sam_current_image = i
        sam_previous_image = None
        
        # Clear SAM memory before processing new image
        if to_segment and predictor is not None:
            try:
                predictor.reset_image()
            except:
                pass  # Some SAM versions don't have reset_image
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()

        for j, _ in enumerate(dna_image_region_props[f"Frame_{i}"]):  # for each cell

            cell_props = frame_props[j]
            # Get the box size for this specific cell
            if isinstance(box_sizes, list):
                current_box_size = box_sizes[j]
            else:
                current_box_size = box_sizes

            y, x = cell_props.centroid
            coords_temp = square_box([y,x], current_box_size)
            x1, y2, x2, y1 = coords_temp


            if (
                all(k >= 0 and k <= dna_image_stack.shape[1] for k in coords_temp)
                == False
            ):
                continue
            else:
                pass

            dna_region = dna_image_stack[i, int(y2) : int(y1), int(x1) : int(x2)]
            dna_regions_temp.append(dna_region)

            og_seg_region = labeled_stack[i, int(y2) : int(y1), int(x1) : int(x2)]
            og_seg_region = (og_seg_region == cell_props.label).astype(np.uint8)
            dna_seg_temp.append(og_seg_region)

            phs_region = phase_image_stack[i, int(y2) : int(y1), int(x1) : int(x2)]
            phs_regions_temp.append(phs_region)

            if to_segment == True:
                if (
                    sam_current_image != sam_previous_image
                    or sam_previous_image == None
                ):
                    # Convert to RGB only when needed and clear after use
                    phase_image_rgb = bw_to_rgb(
                        phase_image_stack[sam_current_image, :, :]
                    )
                    predictor.set_image(phase_image_rgb)
                    sam_previous_image = sam_current_image
                    
                    # Clear the RGB image from memory after setting it in predictor
                    del phase_image_rgb
                    gc.collect()

                if box_prompts == True:

                    if all(
                        k >= 0 and k <= dna_image_stack.shape[1] for k in coords_temp
                    ):
                        boxes.append(coords_temp)
                    else:
                        discarded_box_counter[i] += 1

                    if len(boxes) == batch_size or (j + 1) == len(
                        dna_image_region_props[f"Frame_{i}"]
                    ):
                        current_batch_boxes = boxes.copy()
                        masks = predict(
                            predictor,
                            None,  # Don't pass RGB image again, SAM already has it
                            boxes=boxes,
                            box_prompts=True,
                        )
                        # Append prompts in same order as masks
                        prompts_temp.extend(current_batch_boxes)
                        segmentations_temp = masks
                        boxes = []

                elif point_prompts == True:

                    points = [[x, y]]
                    mask = predict(
                        predictor,
                        None,  # Don't pass RGB image again, SAM already has it
                        points=points,
                        point_prompts=True,
                    )
                    segmentations_temp.append(mask)
                    prompts_temp.append([x, y])

        kept_indices = iou_with_list(
            segmentations_temp, iou_thresh
        )

        discarded_box_counter[i] += (
            len(segmentations_temp) - len(kept_indices)
        )

        segmentations_temp = [
            seg for i, seg in enumerate(segmentations_temp) if i in kept_indices
        ]
        prompts_temp = [
            prm for i, prm in enumerate(prompts_temp) if i in kept_indices
        ]
        dna_regions_temp = [
            roi for i, roi in enumerate(dna_regions_temp) if i in kept_indices
        ]
        phs_regions_temp = [
            roi for i, roi in enumerate(phs_regions_temp) if i in kept_indices
        ]
        dna_seg_temp = [
            roi for i, roi in enumerate(dna_seg_temp) if i in kept_indices
        ]

        segmentations.append(segmentations_temp)
        dna_regions.append(dna_regions_temp)
        phs_regions.append(phs_regions_temp)
        dna_seg.append(dna_seg_temp)
        prompts.append(prompts_temp)

    # Clear large intermediate data structures
    del labeled_stack, dna_image_region_props
    gc.collect()
    
    dtype = object if len(dna_regions) > 1 else 'uint16'
    dna_regions = np.asarray(dna_regions, dtype=dtype)
    phs_regions = np.asarray(phs_regions, dtype=dtype)
    segmentations = np.asarray(segmentations, dtype=dtype)
    dna_seg = np.asarray(dna_seg, dtype=dtype)
    prompts = np.asarray(prompts, dtype=dtype)

    return (dna_regions, discarded_box_counter, segmentations, phs_regions, dna_seg, prompts)


def add_labels(data_frame: npt.NDArray, labels: npt.NDArray) -> npt.NDArray:
    """
    Adds labels to a dataframe when labels and dataframe have the same number of rows.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	data_frame: npt.NDArray, input dataframe
    	labels: npt.NDArray, labels to add to the dataframe
    OUTPUTS:
    	data_frame: npt.NDArray, dataframe with one extra column containing the labels
    """
    if len(labels.shape) == len(data_frame.shape):
        if labels.shape[0] == data_frame.shape[0]:
            data_frame = np.append(data_frame, labels, axis=1)
    else:
        data_frame = np.append(
            data_frame, np.reshape(labels, (data_frame.shape[0], 1)), axis=1
        )

    return data_frame


def binImage(img: npt.NDArray, new_shape: tuple, method: str = "mean") -> npt.NDArray:
    """
    Bins an image to a new shape using specified method.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	img: npt.NDArray, original array to be binned
    	new_shape: tuple, final desired shape of the array
    	method: str, binning method ('min', 'max', or 'mean')
    OUTPUTS:
    	out: npt.NDArray, binned image with the specified new shape
    """
    if len(img.shape) == 3:
        shape = (
            new_shape[0],
            img.shape[0] // new_shape[0],
            new_shape[1],
            img.shape[1] // new_shape[1],
            3,
        )
        index0 = -2
        index1 = 1
    elif len(img.shape) == 2:
        shape = (
            new_shape[0],
            img.shape[0] // new_shape[0],
            new_shape[1],
            img.shape[1] // new_shape[1],
        )
        index0 = -1
        index1 = 1
    else:
        print(
            "Input image must be either RGB like, (3 dimensional) or black and white (2 dimensional)"
        )
        return
    img = img.reshape(shape)
    if method == "min":
        out = img.min(index0).min(index1)
    elif method == "max":
        out = img.max(index0).max(index1)
    elif method == "mean":
        out = img.mean(index0).mean(index1)
    return out


def write_clusters(
    dataframe: npt.NDArray, cluster_coloumn: int
) -> dict[Union[str, int], npt.NDArray]:
    """
    Separates dataframe rows into clusters based on cluster labels.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	dataframe: npt.NDArray, dataframe containing one column that corresponds to cluster labels
    	cluster_coloumn: int, the index of the column that corresponds to the clustering
    OUTPUTS:
    	clusters: dict[Union[str, int], npt.NDArray], dictionary containing cluster numbers as keys and arrays of [frame, cell] coordinates as values
    """

    num_clusters = int(dataframe[:, cluster_coloumn].max() + 1)
    clusters = {i: np.zeros(shape=(0, 2)) for i in range(num_clusters)}
    clusters["noise"] = np.zeros(shape=(0, 2))

    for i in range(dataframe.shape[0]):
        for j in range(num_clusters):
            if dataframe[i, cluster_coloumn] == j:
                cluster_temp = np.asarray([[dataframe[i, -3], dataframe[i, -2]]])
                clusters[j] = np.append(clusters[j], cluster_temp, axis=0)

        if dataframe[i, cluster_coloumn] == -1:
            cluster_temp = np.asarray([[dataframe[i, -3], dataframe[i, -2]]])
            clusters["noise"] = np.append(clusters["noise"], cluster_temp, axis=0)

    return clusters


def square_reshape(img: npt.NDArray, desired_shape: tuple) -> npt.NDArray:
    """
    Reshapes a square image to desired dimensions.
    ------------------------------------------------------------------------------------------------------
    INPUTS:
    	img: npt.NDArray, input image to be reshaped
    	desired_shape: tuple, target shape for the image
    OUTPUTS:
    	img: npt.NDArray, reshaped image with the desired dimensions
    """

    if img.shape[0] < desired_shape[0]:
        qdiff = (2048 - img.shape[0]) // 4
        img = np.pad(
            img,
            [(qdiff, qdiff), (qdiff, qdiff), (0, 0)],
            mode="constant",
            constant_values=img.mean(),
        )
    elif img.shape[0] > desired_shape[0]:
        img = binImage(img, desired_shape)

    return img